{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary\n",
    "from torchviz import make_dot\n",
    "from torch.autograd import Variable\n",
    "from statistics import mean\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "input_size = 32\n",
    "batch_size = 100\n",
    "\n",
    "transform = transforms.Compose([\n",
    " transforms.Pad(4),\n",
    " transforms.RandomHorizontalFlip(),\n",
    " transforms.RandomCrop(32),\n",
    " transforms.ToTensor(),\n",
    " transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)) # only can do to tensor so keep order\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10('C:\\data/cifar10', train=True, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset= train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = datasets.CIFAR10(root='C:\\data/',\n",
    "                                            train=False, \n",
    "                                            transform=transforms.ToTensor())\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.block11 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "\n",
    "        self.block12 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(16)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=2, bias=False)\n",
    "\n",
    "        self.block21 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.block22 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(32)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=2, bias=False)\n",
    "\n",
    "        self.block31 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1, stride=2, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.block32 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(64)\n",
    "        )\n",
    "\n",
    "        self.avg_pool = nn.AvgPool2d(8)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out0 = self.conv0(x)\n",
    "        out1 = self.block11(out0)\n",
    "        out1 = self.block12(out1)\n",
    "\n",
    "        res2 = self.conv2(out1)\n",
    "        out2 = self.block21(out1)\n",
    "        out2 = self.block22(out2)\n",
    "        out2 += res2\n",
    "        out2 = self.relu(out2)\n",
    "\n",
    "        res3 = self.conv3(out2)\n",
    "        out3 = self.block31(out2)\n",
    "        out3 = self.block32(out3)\n",
    "        out3 += res3\n",
    "        out3 = self.relu(out3)\n",
    "\n",
    "        out3 = self.avg_pool(out3)\n",
    "        out3 = self.flatten(out3)\n",
    "        out = self.fc(out3)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleResNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'model.png'"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "InTensor = Variable(torch.randn(1, 3, 32, 32)).to(device)\n",
    "make_dot(model(InTensor), params=dict(model.named_parameters())).render(\"model\", format=\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 16, 32, 32]             432\n       BatchNorm2d-2           [-1, 16, 32, 32]              32\n              ReLU-3           [-1, 16, 32, 32]               0\n            Conv2d-4           [-1, 16, 32, 32]           2,304\n       BatchNorm2d-5           [-1, 16, 32, 32]              32\n              ReLU-6           [-1, 16, 32, 32]               0\n            Conv2d-7           [-1, 16, 32, 32]           2,304\n       BatchNorm2d-8           [-1, 16, 32, 32]              32\n            Conv2d-9           [-1, 16, 32, 32]           2,304\n      BatchNorm2d-10           [-1, 16, 32, 32]              32\n             ReLU-11           [-1, 16, 32, 32]               0\n           Conv2d-12           [-1, 16, 32, 32]           2,304\n      BatchNorm2d-13           [-1, 16, 32, 32]              32\n           Conv2d-14           [-1, 32, 16, 16]           4,608\n           Conv2d-15           [-1, 32, 16, 16]           4,608\n      BatchNorm2d-16           [-1, 32, 16, 16]              64\n             ReLU-17           [-1, 32, 16, 16]               0\n           Conv2d-18           [-1, 32, 16, 16]           9,216\n      BatchNorm2d-19           [-1, 32, 16, 16]              64\n           Conv2d-20           [-1, 32, 16, 16]           9,216\n      BatchNorm2d-21           [-1, 32, 16, 16]              64\n             ReLU-22           [-1, 32, 16, 16]               0\n           Conv2d-23           [-1, 32, 16, 16]           9,216\n      BatchNorm2d-24           [-1, 32, 16, 16]              64\n             ReLU-25           [-1, 32, 16, 16]               0\n           Conv2d-26             [-1, 64, 8, 8]          18,432\n           Conv2d-27             [-1, 64, 8, 8]          18,432\n      BatchNorm2d-28             [-1, 64, 8, 8]             128\n             ReLU-29             [-1, 64, 8, 8]               0\n           Conv2d-30             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-31             [-1, 64, 8, 8]             128\n           Conv2d-32             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-33             [-1, 64, 8, 8]             128\n             ReLU-34             [-1, 64, 8, 8]               0\n           Conv2d-35             [-1, 64, 8, 8]          36,864\n      BatchNorm2d-36             [-1, 64, 8, 8]             128\n             ReLU-37             [-1, 64, 8, 8]               0\n        AvgPool2d-38             [-1, 64, 1, 1]               0\n          Flatten-39                   [-1, 64]               0\n           Linear-40                   [-1, 10]             650\n================================================================\nTotal params: 195,546\nTrainable params: 195,546\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.01\nForward/backward pass size (MB): 2.75\nParams size (MB): 0.75\nEstimated Total Size (MB): 3.51\n----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, input_size=(3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, LambdaLR, StepLR, MultiStepLR, ExponentialLR, CosineAnnealingLR\n",
    "\n",
    "scheduler_list = [\n",
    "    ReduceLROnPlateau(\n",
    "        optimizer=optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=2\n",
    "        ),\n",
    "    LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda epoch: 1 / (epoch+1)\n",
    "        ),\n",
    "    StepLR(\n",
    "        optimizer=optimizer,\n",
    "        step_size=5,\n",
    "        gamma=0.5\n",
    "        ),\n",
    "    MultiStepLR(\n",
    "        optimizer=optimizer,\n",
    "        milestones=[2, 5, 10, 11, 28],\n",
    "        gamma=0.5\n",
    "        ),\n",
    "    ExponentialLR(\n",
    "        optimizer=optimizer,\n",
    "        gamma=0.5\n",
    "        ),\n",
    "    CosineAnnealingLR(\n",
    "        optimizer=optimizer,\n",
    "        T_max=10,\n",
    "        eta_min=0\n",
    "    )\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(\n",
    "    optimizer=optimizer, \n",
    "    mode='min',\n",
    "    factor=0.5,\n",
    "    patience=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch [1/50] Step [100/500] Loss: 1.6568\n",
      "Epoch [1/50] Step [200/500] Loss: 1.5901\n",
      "Epoch [1/50] Step [300/500] Loss: 1.2875\n",
      "Epoch [1/50] Step [400/500] Loss: 1.3420\n",
      "Epoch [1/50] Step [500/500] Loss: 1.2012\n",
      "Epoch [1] Train Loss: 1.5157 Val Loss: 1.2086\n",
      "========================================================================================\n",
      "Epoch [2/50] Step [100/500] Loss: 1.1809\n",
      "Epoch [2/50] Step [200/500] Loss: 1.0576\n",
      "Epoch [2/50] Step [300/500] Loss: 1.1619\n",
      "Epoch [2/50] Step [400/500] Loss: 1.3896\n",
      "Epoch [2/50] Step [500/500] Loss: 1.1292\n",
      "Epoch [2] Train Loss: 1.2049 Val Loss: 1.2077\n",
      "========================================================================================\n",
      "Epoch [3/50] Step [100/500] Loss: 1.1958\n",
      "Epoch [3/50] Step [200/500] Loss: 1.1460\n",
      "Epoch [3/50] Step [300/500] Loss: 1.2286\n",
      "Epoch [3/50] Step [400/500] Loss: 1.1035\n",
      "Epoch [3/50] Step [500/500] Loss: 1.2620\n",
      "Epoch [3] Train Loss: 1.2047 Val Loss: 1.2070\n",
      "========================================================================================\n",
      "Epoch [4/50] Step [100/500] Loss: 1.1618\n",
      "Epoch [4/50] Step [200/500] Loss: 1.2122\n",
      "Epoch [4/50] Step [300/500] Loss: 1.1109\n",
      "Epoch [4/50] Step [400/500] Loss: 1.1217\n",
      "Epoch [4/50] Step [500/500] Loss: 1.0814\n",
      "Epoch [4] Train Loss: 1.2053 Val Loss: 1.2062\n",
      "========================================================================================\n",
      "Epoch [5/50] Step [100/500] Loss: 1.2198\n",
      "Epoch [5/50] Step [200/500] Loss: 1.1751\n",
      "Epoch [5/50] Step [300/500] Loss: 1.2259\n",
      "Epoch [5/50] Step [400/500] Loss: 1.2840\n",
      "Epoch [5/50] Step [500/500] Loss: 1.1720\n",
      "Epoch [5] Train Loss: 1.2009 Val Loss: 1.2055\n",
      "========================================================================================\n",
      "Epoch [6/50] Step [100/500] Loss: 1.2160\n",
      "Epoch [6/50] Step [200/500] Loss: 1.1225\n",
      "Epoch [6/50] Step [300/500] Loss: 1.2002\n",
      "Epoch [6/50] Step [400/500] Loss: 1.0751\n",
      "Epoch [6/50] Step [500/500] Loss: 1.1137\n",
      "Epoch [6] Train Loss: 1.2007 Val Loss: 1.2048\n",
      "========================================================================================\n",
      "Epoch [7/50] Step [100/500] Loss: 1.1937\n",
      "Epoch [7/50] Step [200/500] Loss: 1.3399\n",
      "Epoch [7/50] Step [300/500] Loss: 1.2565\n",
      "Epoch [7/50] Step [400/500] Loss: 1.1190\n",
      "Epoch [7/50] Step [500/500] Loss: 1.0429\n",
      "Epoch [7] Train Loss: 1.1983 Val Loss: 1.2041\n",
      "========================================================================================\n",
      "Epoch [8/50] Step [100/500] Loss: 1.1068\n",
      "Epoch [8/50] Step [200/500] Loss: 1.1397\n",
      "Epoch [8/50] Step [300/500] Loss: 1.1873\n",
      "Epoch [8/50] Step [400/500] Loss: 1.1826\n",
      "Epoch [8/50] Step [500/500] Loss: 1.2358\n",
      "Epoch [8] Train Loss: 1.1989 Val Loss: 1.2034\n",
      "========================================================================================\n",
      "Epoch [9/50] Step [100/500] Loss: 1.1189\n",
      "Epoch [9/50] Step [200/500] Loss: 1.2422\n",
      "Epoch [9/50] Step [300/500] Loss: 1.1839\n",
      "Epoch [9/50] Step [400/500] Loss: 1.0201\n",
      "Epoch [9/50] Step [500/500] Loss: 1.1694\n",
      "Epoch [9] Train Loss: 1.1960 Val Loss: 1.2027\n",
      "========================================================================================\n",
      "Epoch [10/50] Step [100/500] Loss: 1.0501\n",
      "Epoch [10/50] Step [200/500] Loss: 1.2049\n",
      "Epoch [10/50] Step [300/500] Loss: 1.0816\n",
      "Epoch [10/50] Step [400/500] Loss: 1.1185\n",
      "Epoch [10/50] Step [500/500] Loss: 1.3349\n",
      "Epoch [10] Train Loss: 1.1987 Val Loss: 1.2021\n",
      "========================================================================================\n",
      "Epoch [11/50] Step [100/500] Loss: 1.2140\n",
      "Epoch [11/50] Step [200/500] Loss: 1.2094\n",
      "Epoch [11/50] Step [300/500] Loss: 1.3000\n",
      "Epoch [11/50] Step [400/500] Loss: 1.1400\n",
      "Epoch [11/50] Step [500/500] Loss: 1.3473\n",
      "Epoch [11] Train Loss: 1.1966 Val Loss: 1.2014\n",
      "========================================================================================\n",
      "Epoch [12/50] Step [100/500] Loss: 1.1322\n",
      "Epoch [12/50] Step [200/500] Loss: 1.1410\n",
      "Epoch [12/50] Step [300/500] Loss: 1.1040\n",
      "Epoch [12/50] Step [400/500] Loss: 1.1894\n",
      "Epoch [12/50] Step [500/500] Loss: 1.2140\n",
      "Epoch [12] Train Loss: 1.1945 Val Loss: 1.2008\n",
      "========================================================================================\n",
      "Epoch [13/50] Step [100/500] Loss: 1.2374\n",
      "Epoch [13/50] Step [200/500] Loss: 1.1847\n",
      "Epoch [13/50] Step [300/500] Loss: 1.1723\n",
      "Epoch [13/50] Step [400/500] Loss: 1.0625\n",
      "Epoch [13/50] Step [500/500] Loss: 1.1450\n",
      "Epoch [13] Train Loss: 1.1952 Val Loss: 1.2001\n",
      "========================================================================================\n",
      "Epoch [14/50] Step [100/500] Loss: 1.2049\n",
      "Epoch [14/50] Step [200/500] Loss: 1.3159\n",
      "Epoch [14/50] Step [300/500] Loss: 1.1636\n",
      "Epoch [14/50] Step [400/500] Loss: 1.2167\n",
      "Epoch [14/50] Step [500/500] Loss: 1.0474\n",
      "Epoch [14] Train Loss: 1.1944 Val Loss: 1.1995\n",
      "========================================================================================\n",
      "Epoch [15/50] Step [100/500] Loss: 1.2905\n",
      "Epoch [15/50] Step [200/500] Loss: 1.0920\n",
      "Epoch [15/50] Step [300/500] Loss: 1.3685\n",
      "Epoch [15/50] Step [400/500] Loss: 1.2579\n",
      "Epoch [15/50] Step [500/500] Loss: 1.2660\n",
      "Epoch [15] Train Loss: 1.1930 Val Loss: 1.1989\n",
      "========================================================================================\n",
      "Epoch [16/50] Step [100/500] Loss: 1.2744\n",
      "Epoch [16/50] Step [200/500] Loss: 1.1296\n",
      "Epoch [16/50] Step [300/500] Loss: 1.1947\n",
      "Epoch [16/50] Step [400/500] Loss: 1.1233\n",
      "Epoch [16/50] Step [500/500] Loss: 1.1002\n",
      "Epoch [16] Train Loss: 1.1920 Val Loss: 1.1983\n",
      "========================================================================================\n",
      "Epoch [17/50] Step [100/500] Loss: 1.2202\n",
      "Epoch [17/50] Step [200/500] Loss: 1.3518\n",
      "Epoch [17/50] Step [300/500] Loss: 1.1027\n",
      "Epoch [17/50] Step [400/500] Loss: 1.3053\n",
      "Epoch [17/50] Step [500/500] Loss: 1.2843\n",
      "Epoch [17] Train Loss: 1.1917 Val Loss: 1.1977\n",
      "========================================================================================\n",
      "Epoch [18/50] Step [100/500] Loss: 1.2826\n",
      "Epoch [18/50] Step [200/500] Loss: 1.2389\n",
      "Epoch [18/50] Step [300/500] Loss: 1.1349\n",
      "Epoch [18/50] Step [400/500] Loss: 1.1736\n",
      "Epoch [18/50] Step [500/500] Loss: 1.2435\n",
      "Epoch [18] Train Loss: 1.1901 Val Loss: 1.1971\n",
      "========================================================================================\n",
      "Epoch [19/50] Step [100/500] Loss: 1.1972\n",
      "Epoch [19/50] Step [200/500] Loss: 1.3124\n",
      "Epoch [19/50] Step [300/500] Loss: 1.2819\n",
      "Epoch [19/50] Step [400/500] Loss: 1.3201\n",
      "Epoch [19/50] Step [500/500] Loss: 1.1678\n",
      "Epoch [19] Train Loss: 1.1916 Val Loss: 1.1965\n",
      "========================================================================================\n",
      "Epoch [20/50] Step [100/500] Loss: 1.2710\n",
      "Epoch [20/50] Step [200/500] Loss: 1.1292\n",
      "Epoch [20/50] Step [300/500] Loss: 1.1564\n",
      "Epoch [20/50] Step [400/500] Loss: 1.0500\n",
      "Epoch [20/50] Step [500/500] Loss: 1.2720\n",
      "Epoch [20] Train Loss: 1.1951 Val Loss: 1.1959\n",
      "========================================================================================\n",
      "Epoch [21/50] Step [100/500] Loss: 1.1767\n",
      "Epoch [21/50] Step [200/500] Loss: 1.1244\n",
      "Epoch [21/50] Step [300/500] Loss: 1.1583\n",
      "Epoch [21/50] Step [400/500] Loss: 1.2382\n",
      "Epoch [21/50] Step [500/500] Loss: 1.2040\n",
      "Epoch [21] Train Loss: 1.1896 Val Loss: 1.1954\n",
      "========================================================================================\n",
      "Epoch [22/50] Step [100/500] Loss: 1.1873\n",
      "Epoch [22/50] Step [200/500] Loss: 1.0123\n",
      "Epoch [22/50] Step [300/500] Loss: 1.1128\n",
      "Epoch [22/50] Step [400/500] Loss: 1.2891\n",
      "Epoch [22/50] Step [500/500] Loss: 1.1794\n",
      "Epoch [22] Train Loss: 1.1883 Val Loss: 1.1948\n",
      "========================================================================================\n",
      "Epoch [23/50] Step [100/500] Loss: 1.0870\n",
      "Epoch [23/50] Step [200/500] Loss: 1.2143\n",
      "Epoch [23/50] Step [300/500] Loss: 1.1046\n",
      "Epoch [23/50] Step [400/500] Loss: 1.1095\n",
      "Epoch [23/50] Step [500/500] Loss: 1.1146\n",
      "Epoch [23] Train Loss: 1.1867 Val Loss: 1.1943\n",
      "========================================================================================\n",
      "Epoch [24/50] Step [100/500] Loss: 1.0061\n",
      "Epoch [24/50] Step [200/500] Loss: 1.1427\n",
      "Epoch [24/50] Step [300/500] Loss: 1.2390\n",
      "Epoch [24/50] Step [400/500] Loss: 1.1707\n",
      "Epoch [24/50] Step [500/500] Loss: 1.3581\n",
      "Epoch [24] Train Loss: 1.1871 Val Loss: 1.1937\n",
      "========================================================================================\n",
      "Epoch [25/50] Step [100/500] Loss: 1.1142\n",
      "Epoch [25/50] Step [200/500] Loss: 1.1030\n",
      "Epoch [25/50] Step [300/500] Loss: 1.2671\n",
      "Epoch [25/50] Step [400/500] Loss: 1.0473\n",
      "Epoch [25/50] Step [500/500] Loss: 1.2298\n",
      "Epoch [25] Train Loss: 1.1881 Val Loss: 1.1932\n",
      "========================================================================================\n",
      "Epoch [26/50] Step [100/500] Loss: 1.0821\n",
      "Epoch [26/50] Step [200/500] Loss: 1.3818\n",
      "Epoch [26/50] Step [300/500] Loss: 1.1718\n",
      "Epoch [26/50] Step [400/500] Loss: 1.0171\n",
      "Epoch [26/50] Step [500/500] Loss: 1.2304\n",
      "Epoch [26] Train Loss: 1.1858 Val Loss: 1.1927\n",
      "========================================================================================\n",
      "Epoch [27/50] Step [100/500] Loss: 1.1625\n",
      "Epoch [27/50] Step [200/500] Loss: 1.2473\n",
      "Epoch [27/50] Step [300/500] Loss: 1.1684\n",
      "Epoch [27/50] Step [400/500] Loss: 1.1157\n",
      "Epoch [27/50] Step [500/500] Loss: 1.1470\n",
      "Epoch [27] Train Loss: 1.1874 Val Loss: 1.1922\n",
      "========================================================================================\n",
      "Epoch [28/50] Step [100/500] Loss: 1.2409\n",
      "Epoch [28/50] Step [200/500] Loss: 1.0487\n",
      "Epoch [28/50] Step [300/500] Loss: 1.2597\n",
      "Epoch [28/50] Step [400/500] Loss: 1.1562\n",
      "Epoch [28/50] Step [500/500] Loss: 1.2381\n",
      "Epoch [28] Train Loss: 1.1855 Val Loss: 1.1917\n",
      "========================================================================================\n",
      "Epoch [29/50] Step [100/500] Loss: 1.2476\n",
      "Epoch [29/50] Step [200/500] Loss: 1.0781\n",
      "Epoch [29/50] Step [300/500] Loss: 1.0627\n",
      "Epoch [29/50] Step [400/500] Loss: 1.3271\n",
      "Epoch [29/50] Step [500/500] Loss: 1.3794\n",
      "Epoch [29] Train Loss: 1.1845 Val Loss: 1.1912\n",
      "========================================================================================\n",
      "Epoch [30/50] Step [100/500] Loss: 1.0772\n",
      "Epoch [30/50] Step [200/500] Loss: 1.1040\n",
      "Epoch [30/50] Step [300/500] Loss: 1.1538\n",
      "Epoch [30/50] Step [400/500] Loss: 0.8756\n",
      "Epoch [30/50] Step [500/500] Loss: 1.2699\n",
      "Epoch [30] Train Loss: 1.1856 Val Loss: 1.1907\n",
      "========================================================================================\n",
      "Epoch [31/50] Step [100/500] Loss: 1.2919\n",
      "Epoch [31/50] Step [200/500] Loss: 1.2172\n",
      "Epoch [31/50] Step [300/500] Loss: 1.1849\n",
      "Epoch [31/50] Step [400/500] Loss: 1.1464\n",
      "Epoch [31/50] Step [500/500] Loss: 1.1539\n",
      "Epoch [31] Train Loss: 1.1841 Val Loss: 1.1902\n",
      "========================================================================================\n",
      "Epoch [32/50] Step [100/500] Loss: 1.2011\n",
      "Epoch [32/50] Step [200/500] Loss: 1.1114\n",
      "Epoch [32/50] Step [300/500] Loss: 1.0366\n",
      "Epoch [32/50] Step [400/500] Loss: 1.0994\n",
      "Epoch [32/50] Step [500/500] Loss: 1.3704\n",
      "Epoch [32] Train Loss: 1.1834 Val Loss: 1.1897\n",
      "========================================================================================\n",
      "Epoch [33/50] Step [100/500] Loss: 1.2483\n",
      "Epoch [33/50] Step [200/500] Loss: 1.2170\n",
      "Epoch [33/50] Step [300/500] Loss: 1.1952\n",
      "Epoch [33/50] Step [400/500] Loss: 1.1931\n",
      "Epoch [33/50] Step [500/500] Loss: 1.2711\n",
      "Epoch [33] Train Loss: 1.1856 Val Loss: 1.1892\n",
      "========================================================================================\n",
      "Epoch [34/50] Step [100/500] Loss: 1.1657\n",
      "Epoch [34/50] Step [200/500] Loss: 1.3300\n",
      "Epoch [34/50] Step [300/500] Loss: 1.2547\n",
      "Epoch [34/50] Step [400/500] Loss: 1.1272\n",
      "Epoch [34/50] Step [500/500] Loss: 1.3949\n",
      "Epoch [34] Train Loss: 1.1822 Val Loss: 1.1888\n",
      "========================================================================================\n",
      "Epoch [35/50] Step [100/500] Loss: 1.2233\n",
      "Epoch [35/50] Step [200/500] Loss: 1.3364\n",
      "Epoch [35/50] Step [300/500] Loss: 1.2648\n",
      "Epoch [35/50] Step [400/500] Loss: 1.1930\n",
      "Epoch [35/50] Step [500/500] Loss: 1.1214\n",
      "Epoch [35] Train Loss: 1.1813 Val Loss: 1.1883\n",
      "========================================================================================\n",
      "Epoch [36/50] Step [100/500] Loss: 1.1542\n",
      "Epoch [36/50] Step [200/500] Loss: 1.1675\n",
      "Epoch [36/50] Step [300/500] Loss: 1.3514\n",
      "Epoch [36/50] Step [400/500] Loss: 1.0940\n",
      "Epoch [36/50] Step [500/500] Loss: 1.1492\n",
      "Epoch [36] Train Loss: 1.1815 Val Loss: 1.1879\n",
      "========================================================================================\n",
      "Epoch [37/50] Step [100/500] Loss: 1.3640\n",
      "Epoch [37/50] Step [200/500] Loss: 1.0125\n",
      "Epoch [37/50] Step [300/500] Loss: 1.2092\n",
      "Epoch [37/50] Step [400/500] Loss: 1.2011\n",
      "Epoch [37/50] Step [500/500] Loss: 1.1041\n",
      "Epoch [37] Train Loss: 1.1833 Val Loss: 1.1874\n",
      "========================================================================================\n",
      "Epoch [38/50] Step [100/500] Loss: 1.3260\n",
      "Epoch [38/50] Step [200/500] Loss: 1.1772\n",
      "Epoch [38/50] Step [300/500] Loss: 1.0490\n",
      "Epoch [38/50] Step [400/500] Loss: 1.1140\n",
      "Epoch [38/50] Step [500/500] Loss: 1.2522\n",
      "Epoch [38] Train Loss: 1.1837 Val Loss: 1.1870\n",
      "========================================================================================\n",
      "Epoch [39/50] Step [100/500] Loss: 1.1096\n",
      "Epoch [39/50] Step [200/500] Loss: 1.2233\n",
      "Epoch [39/50] Step [300/500] Loss: 1.0275\n",
      "Epoch [39/50] Step [400/500] Loss: 1.0986\n",
      "Epoch [39/50] Step [500/500] Loss: 1.1803\n",
      "Epoch [39] Train Loss: 1.1799 Val Loss: 1.1866\n",
      "========================================================================================\n",
      "Epoch [40/50] Step [100/500] Loss: 1.1148\n",
      "Epoch [40/50] Step [200/500] Loss: 1.2590\n",
      "Epoch [40/50] Step [300/500] Loss: 1.4493\n",
      "Epoch [40/50] Step [400/500] Loss: 1.2401\n",
      "Epoch [40/50] Step [500/500] Loss: 0.9512\n",
      "Epoch [40] Train Loss: 1.1792 Val Loss: 1.1861\n",
      "========================================================================================\n",
      "Epoch [41/50] Step [100/500] Loss: 1.3728\n",
      "Epoch [41/50] Step [200/500] Loss: 1.0468\n",
      "Epoch [41/50] Step [300/500] Loss: 1.2343\n",
      "Epoch [41/50] Step [400/500] Loss: 1.0925\n",
      "Epoch [41/50] Step [500/500] Loss: 1.3234\n",
      "Epoch [41] Train Loss: 1.1814 Val Loss: 1.1857\n",
      "========================================================================================\n",
      "Epoch [42/50] Step [100/500] Loss: 1.2204\n",
      "Epoch [42/50] Step [200/500] Loss: 1.1979\n",
      "Epoch [42/50] Step [300/500] Loss: 1.2128\n",
      "Epoch [42/50] Step [400/500] Loss: 1.1954\n",
      "Epoch [42/50] Step [500/500] Loss: 1.2404\n",
      "Epoch [42] Train Loss: 1.1794 Val Loss: 1.1853\n",
      "========================================================================================\n",
      "Epoch [43/50] Step [100/500] Loss: 1.0583\n",
      "Epoch [43/50] Step [200/500] Loss: 1.2747\n",
      "Epoch [43/50] Step [300/500] Loss: 1.1380\n",
      "Epoch [43/50] Step [400/500] Loss: 1.1823\n",
      "Epoch [43/50] Step [500/500] Loss: 1.2345\n",
      "Epoch [43] Train Loss: 1.1814 Val Loss: 1.1849\n",
      "========================================================================================\n",
      "Epoch [44/50] Step [100/500] Loss: 1.0300\n",
      "Epoch [44/50] Step [200/500] Loss: 1.0759\n",
      "Epoch [44/50] Step [300/500] Loss: 1.0961\n",
      "Epoch [44/50] Step [400/500] Loss: 1.1750\n",
      "Epoch [44/50] Step [500/500] Loss: 1.2711\n",
      "Epoch [44] Train Loss: 1.1793 Val Loss: 1.1845\n",
      "========================================================================================\n",
      "Epoch [45/50] Step [100/500] Loss: 1.0340\n",
      "Epoch [45/50] Step [200/500] Loss: 1.2404\n",
      "Epoch [45/50] Step [300/500] Loss: 1.1555\n",
      "Epoch [45/50] Step [400/500] Loss: 1.1443\n",
      "Epoch [45/50] Step [500/500] Loss: 1.2259\n",
      "Epoch [45] Train Loss: 1.1773 Val Loss: 1.1841\n",
      "========================================================================================\n",
      "Epoch [46/50] Step [100/500] Loss: 1.0979\n",
      "Epoch [46/50] Step [200/500] Loss: 1.0526\n",
      "Epoch [46/50] Step [300/500] Loss: 1.5025\n",
      "Epoch [46/50] Step [400/500] Loss: 1.2356\n",
      "Epoch [46/50] Step [500/500] Loss: 1.1682\n",
      "Epoch [46] Train Loss: 1.1765 Val Loss: 1.1837\n",
      "========================================================================================\n",
      "Epoch [47/50] Step [100/500] Loss: 1.0677\n",
      "Epoch [47/50] Step [200/500] Loss: 1.1359\n",
      "Epoch [47/50] Step [300/500] Loss: 1.3625\n",
      "Epoch [47/50] Step [400/500] Loss: 1.3213\n",
      "Epoch [47/50] Step [500/500] Loss: 1.1237\n",
      "Epoch [47] Train Loss: 1.1737 Val Loss: 1.1833\n",
      "========================================================================================\n",
      "Epoch [48/50] Step [100/500] Loss: 1.2153\n",
      "Epoch [48/50] Step [200/500] Loss: 1.1959\n",
      "Epoch [48/50] Step [300/500] Loss: 1.1137\n",
      "Epoch [48/50] Step [400/500] Loss: 1.1887\n",
      "Epoch [48/50] Step [500/500] Loss: 1.2406\n",
      "Epoch [48] Train Loss: 1.1754 Val Loss: 1.1829\n",
      "========================================================================================\n",
      "Epoch [49/50] Step [100/500] Loss: 1.0574\n",
      "Epoch [49/50] Step [200/500] Loss: 1.2743\n",
      "Epoch [49/50] Step [300/500] Loss: 1.1005\n",
      "Epoch [49/50] Step [400/500] Loss: 1.1182\n",
      "Epoch [49/50] Step [500/500] Loss: 1.0004\n",
      "Epoch [49] Train Loss: 1.1756 Val Loss: 1.1826\n",
      "========================================================================================\n",
      "Epoch [50/50] Step [100/500] Loss: 1.1139\n",
      "Epoch [50/50] Step [200/500] Loss: 1.0553\n",
      "Epoch [50/50] Step [300/500] Loss: 1.2088\n",
      "Epoch [50/50] Step [400/500] Loss: 1.2694\n",
      "Epoch [50/50] Step [500/500] Loss: 1.0648\n",
      "Epoch [50] Train Loss: 1.1727 Val Loss: 1.1822\n",
      "========================================================================================\n"
     ]
    }
   ],
   "source": [
    "loss_dict = {}\n",
    "val_loss_dict = {}\n",
    "train_step = len(train_loader)\n",
    "val_step = len(valid_loader)\n",
    "epochs = 50\n",
    "\n",
    "for i in range(1, epochs + 1):\n",
    "    loss_list = [] # losses of i'th epoch\n",
    "    for train_step_idx, (img, label) in enumerate(train_loader):\n",
    "        \n",
    "        img = img.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        output = model(img)\n",
    "        loss = loss_fn(output, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        if ((train_step_idx+1) % 100 == 0):\n",
    "            print(f\"Epoch [{i}/{epochs}] Step [{train_step_idx + 1}/{train_step}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "    loss_dict[i] = loss_list\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_loss_list = []\n",
    "        for val_step_idx, (val_img, val_label) in enumerate(valid_loader):\n",
    "            val_img = val_img.to(device)\n",
    "            val_label = val_label.to(device)\n",
    "\n",
    "            val_output = model(val_img)\n",
    "            val_loss = loss_fn(val_output, val_label)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            val_loss_list.append(val_loss.item())\n",
    "\n",
    "        val_loss_dict[i] = val_loss_list\n",
    "        \n",
    "        # best_loss = mean(val_loss_dict[i])\n",
    "        torch.save({\n",
    "            f\"epoch\": i,\n",
    "            f\"model_state_dict\": model.state_dict(),\n",
    "            f\"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            f\"loss\": loss},\n",
    "             f\"checkpoint/resnet_cifar10_checkpoint_epoch_{i}.ckpt\")\n",
    "\n",
    "\n",
    "    print(f\"Epoch [{i}] Train Loss: {mean(loss_dict[i]):.4f} Val Loss: {mean(val_loss_dict[i]):.4f}\")\n",
    "    print(\"========================================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'resnet.pt')"
   ]
  }
 ]
}